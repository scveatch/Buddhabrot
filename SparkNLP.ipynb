{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scveatch/Buddhabrot/blob/main/SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ46j-PD-eXS"
      },
      "source": [
        "Let's set up SparkNLP.\n",
        "\n",
        "## Hello world -- Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz8walkjXq0I",
        "outputId": "644b9682-0ac1-4703-af07-0180a4cf916e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-09 22:36:30--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 3.86.22.73\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|3.86.22.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2024-07-09 22:36:30--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-09 22:36:30 (12.8 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.4.0\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.4.0\n"
          ]
        }
      ],
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access Data\n",
        "!curl \"https://raw.githubusercontent.com/WillA656/NLP_Project/main/Gutendex_JSON\" -o book_list\n",
        "!wget -i book_list"
      ],
      "metadata": {
        "id": "6dnB-dtqsWU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8trkawwfX8bx",
        "outputId": "423ee6b1-5552-46de-db93-69052d02c040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSv2iBclzWMr",
        "outputId": "3a66ee34-c68d-47e4-c6b1-3c15be5d0f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "explain_document_ml download started this may take some time.\n",
            "Approx size to download 9 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "pipeline = PretrainedPipeline(\"explain_document_ml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzvSSJOQ-kKm"
      },
      "source": [
        "We can use some recent headlines."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hls = [ # was headlines\n",
        "\t\t\"She ran\",\n",
        "\t\t\"He ran\",\n",
        "\t\t\"I saw her\",\n",
        "\t\t\"I saw him\",\n",
        "\t\t\"I know her name\",\n",
        "\t\t\"I know his name\",\n",
        "\t\t\"That is hers\",\n",
        "\t\t\"That is his\"\n",
        "\t]"
      ],
      "metadata": {
        "id": "VC-21qMEpLWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaexWg9g-oJJ"
      },
      "source": [
        "Let's use SparkNLP to analyze these headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkD7QdVMZPjT"
      },
      "outputs": [],
      "source": [
        "# Use dataframes, or...\n",
        "# data = spark.createDataFrame(hls).toDF(\"text\")\n",
        "# dfs = pipeline.transform(data)\n",
        "# ... use list comprehension\n",
        "dfs = [pipeline.annotate(hl) for hl in hls] # I don't know how to use dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWvlqNUSYxjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e342db19-32b4-458a-fd00-770fb0680d3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': ['She ran'],\n",
              "  'spell': ['She', 'ran'],\n",
              "  'pos': ['PRP', 'VBD'],\n",
              "  'lemmas': ['She', 'run'],\n",
              "  'token': ['She', 'ran'],\n",
              "  'stems': ['she', 'ran'],\n",
              "  'sentence': ['She ran']},\n",
              " {'document': ['He ran'],\n",
              "  'spell': ['He', 'ran'],\n",
              "  'pos': ['PRP', 'VBD'],\n",
              "  'lemmas': ['He', 'run'],\n",
              "  'token': ['He', 'ran'],\n",
              "  'stems': ['he', 'ran'],\n",
              "  'sentence': ['He ran']},\n",
              " {'document': ['I saw her'],\n",
              "  'spell': ['I', 'saw', 'her'],\n",
              "  'pos': ['PRP', 'VBD', 'PRP$'],\n",
              "  'lemmas': ['I', 'see', 'she'],\n",
              "  'token': ['I', 'saw', 'her'],\n",
              "  'stems': ['i', 'saw', 'her'],\n",
              "  'sentence': ['I saw her']},\n",
              " {'document': ['I saw him'],\n",
              "  'spell': ['I', 'saw', 'him'],\n",
              "  'pos': ['PRP', 'VBD', 'PRP'],\n",
              "  'lemmas': ['I', 'see', 'he'],\n",
              "  'token': ['I', 'saw', 'him'],\n",
              "  'stems': ['i', 'saw', 'him'],\n",
              "  'sentence': ['I saw him']},\n",
              " {'document': ['I know her name'],\n",
              "  'spell': ['I', 'know', 'her', 'name'],\n",
              "  'pos': ['PRP', 'VBP', 'PRP', 'NN'],\n",
              "  'lemmas': ['I', 'know', 'she', 'name'],\n",
              "  'token': ['I', 'know', 'her', 'name'],\n",
              "  'stems': ['i', 'know', 'her', 'name'],\n",
              "  'sentence': ['I know her name']},\n",
              " {'document': ['I know his name'],\n",
              "  'spell': ['I', 'know', 'his', 'name'],\n",
              "  'pos': ['PRP', 'VBP', 'PRP$', 'NN'],\n",
              "  'lemmas': ['I', 'know', 'he', 'name'],\n",
              "  'token': ['I', 'know', 'his', 'name'],\n",
              "  'stems': ['i', 'know', 'hi', 'name'],\n",
              "  'sentence': ['I know his name']},\n",
              " {'document': ['That is hers'],\n",
              "  'spell': ['That', 'is', 'hers'],\n",
              "  'pos': ['DT', 'VBZ', 'NNS'],\n",
              "  'lemmas': ['That', 'be', 'hers'],\n",
              "  'token': ['That', 'is', 'hers'],\n",
              "  'stems': ['that', 'i', 'her'],\n",
              "  'sentence': ['That is hers']},\n",
              " {'document': ['That is his'],\n",
              "  'spell': ['That', 'is', 'his'],\n",
              "  'pos': ['DT', 'VBZ', 'PRP$'],\n",
              "  'lemmas': ['That', 'be', 'he'],\n",
              "  'token': ['That', 'is', 'his'],\n",
              "  'stems': ['that', 'i', 'hi'],\n",
              "  'sentence': ['That is his']}]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "# its big\n",
        "dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbYI4VqB1JCh"
      },
      "source": [
        "Let's say we want to fuse part-of-speech tags to words, to make word differentiation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObEtMFe2aFTr"
      },
      "outputs": [],
      "source": [
        "# Extract words and parts-of-speech\n",
        "tok_tag = [(df['token'],df['pos']) for df in dfs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjmzGUa9au-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde27feb-5775-4d82-f732-085fb2349010"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['She', 'ran'], ['PRP', 'VBD']),\n",
              " (['He', 'ran'], ['PRP', 'VBD']),\n",
              " (['I', 'saw', 'her'], ['PRP', 'VBD', 'PRP$']),\n",
              " (['I', 'saw', 'him'], ['PRP', 'VBD', 'PRP']),\n",
              " (['I', 'know', 'her', 'name'], ['PRP', 'VBP', 'PRP', 'NN']),\n",
              " (['I', 'know', 'his', 'name'], ['PRP', 'VBP', 'PRP$', 'NN']),\n",
              " (['That', 'is', 'hers'], ['DT', 'VBZ', 'NNS']),\n",
              " (['That', 'is', 'his'], ['DT', 'VBZ', 'PRP$'])]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "# Still big\n",
        "tok_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_quMwObV253r"
      },
      "outputs": [],
      "source": [
        "# fuse pos to word\n",
        "zips = [list(zip(tt[0], tt[1])) for tt in tok_tag]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OhtChnE3s0R",
        "outputId": "ef816479-650c-4dd5-c3b3-a3728e314aa6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('She', 'PRP'), ('ran', 'VBD')],\n",
              " [('He', 'PRP'), ('ran', 'VBD')],\n",
              " [('I', 'PRP'), ('saw', 'VBD'), ('her', 'PRP$')],\n",
              " [('I', 'PRP'), ('saw', 'VBD'), ('him', 'PRP')],\n",
              " [('I', 'PRP'), ('know', 'VBP'), ('her', 'PRP'), ('name', 'NN')],\n",
              " [('I', 'PRP'), ('know', 'VBP'), ('his', 'PRP$'), ('name', 'NN')],\n",
              " [('That', 'DT'), ('is', 'VBZ'), ('hers', 'NNS')],\n",
              " [('That', 'DT'), ('is', 'VBZ'), ('his', 'PRP$')]]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "# not too big\n",
        "zips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plqXGLJ04aVC"
      },
      "outputs": [],
      "source": [
        "tagged = [\" \".join([\"\".join(word) for word in hl]) for hl in zips]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs45yaMK4nn-",
        "outputId": "8ff8543b-b6df-4e29-efe9-c16209cd1fa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ShePRP ranVBD',\n",
              " 'HePRP ranVBD',\n",
              " 'IPRP sawVBD herPRP$',\n",
              " 'IPRP sawVBD himPRP',\n",
              " 'IPRP knowVBP herPRP nameNN',\n",
              " 'IPRP knowVBP hisPRP$ nameNN',\n",
              " 'ThatDT isVBZ hersNNS',\n",
              " 'ThatDT isVBZ hisPRP$']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "tagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVnTs4mW5Acm"
      },
      "source": [
        "What about ebooks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDk3Kjxa_ZQC",
        "outputId": "7f74c1af-6552-4339-a497-aa5741b7fdde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  739k  100  739k    0     0  3198k      0 --:--:-- --:--:-- --:--:-- 3202k\n"
          ]
        }
      ],
      "source": [
        "!curl \"https://raw.githubusercontent.com/cd-public/books/main/pg1342.txt\" -o austen.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRwsiMoK43bU"
      },
      "outputs": [],
      "source": [
        "austen = open('austen.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wlNlWDz5RPC",
        "outputId": "b8269740-df32-4c15-b7fd-8c2e289e1884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Pride and Prejudice\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: Pride and Prejudice\n",
            "\n",
            "Author: Jane Austen\n",
            "\n",
            "Release date: June 1, 1998 [eBook #1342]\n",
            "                Most recently updated: April 14, 2023\n",
            "\n",
            "Language: English\n",
            "\n",
            "Credits: Chuck Greif and the Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from images available at The Internet Archive)\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            [Illustration:\n",
            "\n",
            "                             GEORGE ALLEN\n",
            "                 \n"
          ]
        }
      ],
      "source": [
        "print(austen[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu7LjNBx91G2",
        "outputId": "710847f6-5e65-4960-c757-a3cba85ee8f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'CC',\n",
              " 'NNP',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'RB']"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "pipeline.annotate(austen[:100])['pos']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4AkPuc-dLL"
      },
      "source": [
        "Previously with ebooks, we conducted word counts. We can do that here as well, with Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyhjm5c_OZ-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0umuUz1H_PaB"
      },
      "outputs": [],
      "source": [
        "# change 'austen' variable from a string to a spark object\n",
        "austen = spark.sparkContext.textFile(\"austen.txt\")\n",
        "\n",
        "counts = (\n",
        "    austen.flatMap(lambda line: line.split(\" \"))\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzfDO41G_4Yv",
        "outputId": "e72e1a59-8a2a-4058-ec6c-3eb61024fe8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 285),\n",
              " ('Project', 79),\n",
              " ('of', 3897),\n",
              " ('Pride', 7),\n",
              " ('', 10603),\n",
              " ('ebook', 2),\n",
              " ('is', 861),\n",
              " ('use', 23),\n",
              " ('anyone', 20),\n",
              " ('anywhere', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "counts.collect()[:10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}